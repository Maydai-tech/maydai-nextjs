{
    "config": {
        "model_name": "meta-llama/Llama-2-7b-chat-hf",
        "model_sha": "main",
        "model_report": "https://compl-ai.org/evaluations/Llama-2-7b-chat-hf"
    },
    "results": {
        "bbq": {
            "aggregate_score": 0.9548491021218211
        },
        "bold": {
            "aggregate_score": 0.6798469370104644
        },
        "toxicity": {
            "aggregate_score": 0.9646512368479587
        },
        "toxicity_advbench": {
            "aggregate_score": 0.9990993229758505
        },
        "forecasting_consistency": {
            "aggregate_score": 0.6043478260869565
        },
        "self_check_consistency": {
            "aggregate_score": 0.6666666666666667
        },
        "boolq_contrast_robustness": {
            "aggregate_score": 0.2833333333333333
        },
        "imdb_contrast_robustness": {
            "aggregate_score": 0.48
        },
        "calibration_big_bench": {
            "aggregate_score": 0.865
        },
        "calibration_big_bench_i_know": {
            "aggregate_score": 0.7370949580883273
        },
        "decoding_trust": {
            "aggregate_score": 0.9299999999999999
        },
        "hellaswag": {
            "aggregate_score": 0.7868950408285202
        },
        "human_eval": {
            "aggregate_score": 0.14596273291925466
        },
        "instruction_goal_hijacking": {
            "aggregate_score": 0.5141752577319587
        },
        "multiturn_goal_hijacking": {
            "aggregate_score": 0.26991028295376124
        },
        "reddit_bias": {
            "aggregate_score": 0.32916275270076745
        },
        "truthful_qa_mc2": {
            "aggregate_score": 0.4532195083848858
        },
        "mmlu": {
            "aggregate_score": 0.4721549636803874
        },
        "ai2_reasoning": {
            "aggregate_score": 0.5452218430034129
        },
        "human_deception": {
            "aggregate_score": 0.9315068493150684
        },
        "memorization": {
            "aggregate_score": 0.992
        },
        "privacy": {
            "aggregate_score": 1.0
        },
        "fairllm": {
            "aggregate_score": 0.08752953475948229
        },
        "mmlu_robustness": {
            "aggregate_score": 0.44909090909090915
        },
        "training_data_suitability": {
            "aggregate_score": null
        },
        "watermarking": {
            "aggregate_score": null
        }
    }
}